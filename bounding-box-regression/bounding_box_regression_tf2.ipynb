{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:31:36.150020Z",
     "start_time": "2020-10-06T12:31:35.549804Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/c/Users/81908/jupyter_notebook/tf_2_work/PyImageSearch/Bounding_box_regression\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\81908\\\\Anaconda3\\\\envs\\\\tfgpu\\\\python.exe'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pwd\n",
    "import sys\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "sys.executable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object detection: Bounding box regression with Keras, TensorFlow, and Deep Learning\n",
    "- https://www.pyimagesearch.com/2020/10/05/object-detection-bounding-box-regression-with-keras-tensorflow-and-deep-learning/\n",
    "- 2020/10の時点では、データとソースコード有料になってたので実行できない。ソースの流れのみ記述する\n",
    "\n",
    "<br>\n",
    "\n",
    "### predictのみはcolabで公開されてた\n",
    "- https://colab.research.google.com/drive/1BaKbEokJF1wJ9peHnYath9Bzx2-dGbfE#scrollTo=EC3NrjWVyXZE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:32:41.810734Z",
     "start_time": "2020-10-06T12:32:41.772058Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "import os\n",
    "\n",
    "# 入力データセットの基本パスを定義し，それを利用して画像ディレクトリと注釈CSVファイルへのパスを導出\n",
    "BASE_PATH = \"dataset\"\n",
    "IMAGES_PATH = os.path.sep.join([BASE_PATH, \"images\"])\n",
    "ANNOTS_PATH = os.path.sep.join([BASE_PATH, \"airplanes.csv\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:32:58.395920Z",
     "start_time": "2020-10-06T12:32:58.356927Z"
    }
   },
   "outputs": [],
   "source": [
    "# define the path to the base output directory\n",
    "BASE_OUTPUT = \"output\"\n",
    "\n",
    "# 出力される直列化モデル、モデルトレーニングプロット、テスト画像ファイル名へのパスを定義\n",
    "MODEL_PATH = os.path.sep.join([BASE_OUTPUT, \"detector.h5\"])\n",
    "PLOT_PATH = os.path.sep.join([BASE_OUTPUT, \"plot.png\"])\n",
    "TEST_FILENAMES = os.path.sep.join([BASE_OUTPUT, \"test_images.txt\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:33:12.587893Z",
     "start_time": "2020-10-06T12:33:12.549993Z"
    }
   },
   "outputs": [],
   "source": [
    "# initialize our initial learning rate, number of epochs to train\n",
    "# for, and the batch size\n",
    "INIT_LR = 1e-4\n",
    "NUM_EPOCHS = 25\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train.py\n",
    "### 処理の流れは以下\n",
    "- 飛行機のトレーニングデータをディスクからロードする（つまり、クラスラベルとバウンディングボックスの座標の両方）\n",
    "- ディスクからVGG16をロードし（ImageNetで事前トレーニング済み）、ネットワークから完全に接続された分類レイヤーヘッドを削除し、バウンディングボックス回帰レイヤーヘッドを挿入します\n",
    "- トレーニングデータのバウンディングボックス回帰レイヤーヘッドを微調整する\n",
    "\n",
    "<br>\n",
    "\n",
    "- $ python train.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:34:48.831959Z",
     "start_time": "2020-10-06T12:34:43.440977Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "#from pyimagesearch import config\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:35:06.010323Z",
     "start_time": "2020-10-06T12:35:05.874685Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] loading dataset...\n"
     ]
    }
   ],
   "source": [
    "# load the contents of the CSV annotations file\n",
    "print(\"[INFO] loading dataset...\")\n",
    "#rows = open(config.ANNOTS_PATH).read().strip().split(\"\\n\")\n",
    "\n",
    "# データ（画像）のリストを初期化し，出力予測値（バウンディングボックスの座標）を，個々の画像のファイル名とともに出力\n",
    "data = []\n",
    "targets = []\n",
    "filenames = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bounding box annotations CSV data\n",
    "- image_0001.jpg,49,30,349,137\n",
    "- image_0002.jpg,59,35,342,153\n",
    "\n",
    "<br>\n",
    "\n",
    "#### アノテーションcsvの列は以下の内容記述（物体検知のみなのでラベルなし）\n",
    "- Filename\n",
    "- Starting x-coordinate\n",
    "- Starting y-coordinate\n",
    "- Ending x-coordinate\n",
    "- Ending y-coordinate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop over the rows\n",
    "for row in rows:\n",
    "    # break the row into the filename and bounding box coordinates\n",
    "    row = row.split(\",\")\n",
    "    (filename, startX, startY, endX, endY) = row\n",
    "    \n",
    "    # 入力画像へのパスを導出し，（OpenCV形式で）画像を読み込み，その寸法を取得\n",
    "    imagePath = os.path.sep.join([config.IMAGES_PATH, filename])\n",
    "    image = cv2.imread(imagePath)\n",
    "    (h, w) = image.shape[:2]\n",
    "    \n",
    "    # 入力画像の空間寸法に対するバウンディングボックスの座標を拡大縮小\n",
    "    startX = float(startX) / w\n",
    "    startY = float(startY) / h\n",
    "    endX = float(endX) / w\n",
    "    endY = float(endY) / h\n",
    "    \n",
    "    # load the image and preprocess it\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image)\n",
    "    \n",
    "    # update our list of data, targets, and filenames\n",
    "    data.append(image)\n",
    "    targets.append((startX, startY, endX, endY))\n",
    "    filenames.append(filename)\n",
    "    \n",
    "# データとターゲットを NumPy 配列に変換し，入力ピクセル強度を [0, 255] から [0, 1] の範囲でスケーリング\n",
    "data = np.array(data, dtype=\"float32\") / 255.0\n",
    "targets = np.array(targets, dtype=\"float32\")\n",
    "\n",
    "# 90%のデータを学習用に、残りの10%のデータをテスト用に使用して、データを学習用とテスト用に分割\n",
    "split = train_test_split(data, targets, filenames, test_size=0.10, random_state=42)\n",
    "\n",
    "# unpack the data split\n",
    "(trainImages, testImages) = split[:2]\n",
    "(trainTargets, testTargets) = split[2:4]\n",
    "(trainFilenames, testFilenames) = split[4:]\n",
    "\n",
    "# バウンディングボックスリグレグレッサーを評価/テストするときに使えるように、テスト用のファイル名をディスクに書き出します\n",
    "print(\"[INFO] saving testing filenames...\")\n",
    "f = open(config.TEST_FILENAMES, \"w\")\n",
    "f.write(\"\\n\".join(testFilenames))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### vgg16の出力層けして、4nodeの出力層（x,yの座標）に変更"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:46:54.178624Z",
     "start_time": "2020-10-06T12:46:49.882617Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               3211392   \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                8256      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 17,936,548\n",
      "Trainable params: 3,221,860\n",
      "Non-trainable params: 14,714,688\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# load the VGG16 network, ensuring the head FC layers are left off\n",
    "vgg = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n",
    "\n",
    "# freeze all VGG layers so they will *not* be updated during the\n",
    "# training process\n",
    "vgg.trainable = False\n",
    "\n",
    "# flatten the max-pooling output of VGG\n",
    "flatten = vgg.output\n",
    "flatten = Flatten()(flatten)\n",
    "\n",
    "# construct a fully-connected layer header to output the predicted\n",
    "# bounding box coordinates\n",
    "bboxHead = Dense(128, activation=\"relu\")(flatten)\n",
    "bboxHead = Dense(64, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(32, activation=\"relu\")(bboxHead)\n",
    "bboxHead = Dense(4, activation=\"sigmoid\")(bboxHead)\n",
    "\n",
    "# construct the model we will fine-tune for bounding box regression\n",
    "model = Model(inputs=vgg.input, outputs=bboxHead)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 座標の回帰なのでmse でfit 実行 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize the optimizer, compile the model, and show the model summary\n",
    "opt = Adam(lr=config.INIT_LR)\n",
    "model.compile(loss=\"mse\", optimizer=opt)\n",
    "print(model.summary())\n",
    "\n",
    "# train the network for bounding box regression\n",
    "print(\"[INFO] training bounding box regressor...\")\n",
    "H = model.fit(\n",
    "    trainImages, trainTargets,\n",
    "    validation_data=(testImages, testTargets),\n",
    "    batch_size=config.BATCH_SIZE,\n",
    "    epochs=config.NUM_EPOCHS,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "モデル保存と学習曲線plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# serialize the model to disk\n",
    "print(\"[INFO] saving object detector model...\")\n",
    "model.save(config.MODEL_PATH, save_format=\"h5\")\n",
    "\n",
    "# plot the model training history\n",
    "N = config.NUM_EPOCHS\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, N), H.history[\"loss\"], label=\"train_loss\")\n",
    "plt.plot(np.arange(0, N), H.history[\"val_loss\"], label=\"val_loss\")\n",
    "plt.title(\"Bounding Box Regression Loss on Training Set\")\n",
    "plt.xlabel(\"Epoch #\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc=\"lower left\")\n",
    "plt.savefig(config.PLOT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## forecast.py\n",
    "- 予測実行用py\n",
    "\n",
    "- $ python predict.py --input dataset/images/image_0697.jpg\n",
    "\n",
    "- $ python predict.py --input output/test_images.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:47:54.505287Z",
     "start_time": "2020-10-06T12:47:54.363741Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "#from pyimagesearch import config\n",
    "from tensorflow.keras.preprocessing.image import img_to_array\n",
    "from tensorflow.keras.preprocessing.image import load_img\n",
    "from tensorflow.keras.models import load_model\n",
    "import numpy as np\n",
    "import mimetypes\n",
    "import argparse\n",
    "import imutils\n",
    "import cv2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-10-06T12:48:20.777090Z",
     "start_time": "2020-10-06T12:48:20.639458Z"
    }
   },
   "outputs": [],
   "source": [
    "# construct the argument parser and parse the arguments\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"-i\", \"--input\", required=True, help=\"path to input image/text file of image filenames\")\n",
    "args = vars(ap.parse_args())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 入力ファイルの種類を決定、単一の入力画像を処理していると仮定\n",
    "filetype = mimetypes.guess_type(args[\"input\"])[0]\n",
    "imagePaths = [args[\"input\"]]\n",
    "\n",
    "# ファイルタイプがテキストファイルの場合、複数の*画像を処理する必要があります。\n",
    "if \"text/plain\" == filetype:\n",
    "    \n",
    "    # テストファイルのファイル名を読み込み、画像パスのリストを初期化します。\n",
    "    filenames = open(args[\"input\"]).read().strip().split(\"\\n\")\n",
    "    imagePaths = []\n",
    "    \n",
    "    # ファイル名をループ\n",
    "    for f in filenames:\n",
    "        # 画像ファイル名へのフルパスを作成し、画像パスリストを更新\n",
    "        p = os.path.sep.join([config.IMAGES_PATH, f])\n",
    "        imagePaths.append(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our trained bounding box regressor from disk\n",
    "print(\"[INFO] loading object detector...\")\n",
    "model = load_model(config.MODEL_PATH)\n",
    "\n",
    "# バウンディングボックス回帰モデルを使用してテストする画像をループ\n",
    "for imagePath in imagePaths:\n",
    "    \n",
    "    # 入力画像（Keras形式）をディスクから読み込み，前処理を行い，ピクセル強度を[0, 1]の範囲にスケーリング\n",
    "    image = load_img(imagePath, target_size=(224, 224))\n",
    "    image = img_to_array(image) / 255.0\n",
    "    image = np.expand_dims(image, axis=0)\n",
    "    \n",
    "    # 入力画像に対してバウンディングボックス予測\n",
    "    preds = model.predict(image)[0]\n",
    "    (startX, startY, endX, endY) = preds\n",
    "    \n",
    "    # 入力画像を読み込み（OpenCV形式），画面に収まるようにリサイズし，その寸法を取\n",
    "    image = cv2.imread(imagePath)\n",
    "    image = imutils.resize(image, width=600)\n",
    "    (h, w) = image.shape[:2]\n",
    "    \n",
    "    # 画像の寸法に基づいて，予測されたバウンディングボックスの座標をスケーリング\n",
    "    startX = int(startX * w)\n",
    "    startY = int(startY * h)\n",
    "    endX = int(endX * w)\n",
    "    endY = int(endY * h)\n",
    "    \n",
    "    # 画像上に予測される外接枠を描画\n",
    "    cv2.rectangle(image, (startX, startY), (endX, endY), (0, 255, 0), 2)\n",
    "    \n",
    "    # 出力画像を表示\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "jupytext": {
   "text_representation": {
    "extension": ".py",
    "format_name": "light",
    "format_version": "1.5",
    "jupytext_version": "1.5.2"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
